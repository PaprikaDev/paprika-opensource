{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90858b08",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceefa559-cf67-4cbb-970d-84661c36c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "env = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7bb8f8-33b5-404b-984a-77e515536ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertwalling/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Image\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3185a17",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c130533c-6cd0-414d-ad28-623e4ab6735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Dict, Any\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc3923a2-c5d3-4384-9d74-f98813334c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Tokenize the pdf \\n@tool(\"tokenize-pdf\", args_schema=tokenizerInput, return_direct=True)\\ndef tokenize_pdf():\\n    pass\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download PDF tool\n",
    "from langchain.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from bs4 import BeautifulSoup\n",
    "import tiktoken\n",
    "import requests\n",
    "\n",
    "# Search for relevant menu\n",
    "search_tool = TavilySearchResults()\n",
    "\n",
    "\n",
    "# Scrape menu data\n",
    "class ScrapeInput(BaseModel):\n",
    "    url: str = Field(description=\"the URL of the menu page\")\n",
    "\n",
    "@tool(\"scrape_pdf\", args_schema=ScrapeInput, return_direct=True)\n",
    "def scrape_pdf(url: str):\n",
    "    \"\"\"Scrape a webpage that may include links to a restaurants current menu and return the links\"\"\"\n",
    "    pdf_links = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        pdf_links = [link.get('href') for link in links if link.get('href').endswith('.pdf')]\n",
    "    except Exception as e:\n",
    "        print(f\"failed to scrape {url} ERROR: {e}\")\n",
    "    return pdf_links\n",
    "\n",
    "@tool(\"scrape_text\", args_schema=ScrapeInput, return_direct=True)\n",
    "def scrape_text(url: str):\n",
    "    \"\"\"Scrape the text directly from a website\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ')\n",
    "        # Break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"failed to scrape {url} ERROR: {e}\")\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "# Encode & Upsert menu data\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "class tokenizerInput(BaseModel):\n",
    "    pdf_url: str = Field(description=\"The url of the PDF file that must be tokenized\")\n",
    "\n",
    "\"\"\"\n",
    "# Tokenize the pdf \n",
    "@tool(\"tokenize-pdf\", args_schema=tokenizerInput, return_direct=True)\n",
    "def tokenize_pdf():\n",
    "    pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b4ee54d-15aa-466b-85ca-eab668d1f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool, scrape_pdf, scrape_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be6353-a2a7-4c56-aad4-7afc3a42efd1",
   "metadata": {},
   "source": [
    "# RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4005b8-599b-4a87-8a68-5182b5d93bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # Define the model\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.openai_inference)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    # Check if llm requires action \n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    # Run a tool ordered by the model\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n",
    "    \n",
    "    # Invokes the current message chain\n",
    "    def openai_inference(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a1fe975-3bbc-4814-b979-048899d6d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are tasked with finding and storing the text content of a given restaurants Menu. \n",
    "The menu may be a PDF file that you will be required to scrape, or it will be text on a menu page. \n",
    "ONLY scrape menu data, this includes foods that the restaurant offers and DOES NOT include information about the restaurant itself or the menu iteself.\n",
    "Use the search tool to find the url of the menu, if you cannot find the pdf url directly use the scrape_pdf tool to scrape webpages for the url.\n",
    "If the menu isn't saved as a PDF, and is instead saved as text on a webpage, use the scrape_text tool to gather the text from the page.\n",
    "DO NOT attempt to download blank menus. Save the file using the PDF download tool, name the PDF after the restaurant.\n",
    "Once you have downloaded the PDF there are no more tasks to be done. \n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "\n",
    "model = Model(llm, tools, system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "443ad031-e7ff-410f-baf1-bf70c1d7cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'CRISP Rochester menu'}, 'id': 'call_HXfS5iQGmPo9WPH0T7QvXSvA', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "Calling: {'name': 'scrape_pdf', 'args': {'url': 'https://crisprochester.com/menus/'}, 'id': 'call_TZTEz2dmswggrm36EFMeLURz', 'type': 'tool_call'}\n",
      "Calling: {'name': 'scrape_pdf', 'args': {'url': 'https://crisprochester.com/wp-content/uploads/Lunch-and-Dinner.pdf'}, 'id': 'call_y2x4hbhHrvZ9U1vcGIL2mHyr', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'CRISP Rochester menu PDF'}, 'id': 'call_h4Svl59jiOnk3RSargFx9WkB', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "Calling: {'name': 'download_pdf', 'args': {'file_url': 'https://crisprochester.com/wp-content/uploads/Lunch-and-Dinner.pdf', 'file_name': 'CRISP_Rochester_Lunch_and_Dinner'}, 'id': 'call_aXzqqPR8Ol5Kggx7aXsi0zWc', 'type': 'tool_call'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'download_pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRISP Rochester\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1691\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1690\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1691\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1693\u001b[0m     config,\n\u001b[1;32m   1694\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1695\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1696\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1697\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1698\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1699\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1701\u001b[0m ):\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1703\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1110\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1780\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1778\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1779\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1783\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:52\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/retry.py:72\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     70\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:2822\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2818\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2819\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2820\u001b[0m )\n\u001b[1;32m   2821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2822\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2824\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[18], line 29\u001b[0m, in \u001b[0;36mModel.take_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tool_calls:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     30\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(ToolMessage(tool_call_id\u001b[38;5;241m=\u001b[39mt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], name\u001b[38;5;241m=\u001b[39mt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(result)))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBack to the model!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'download_pdf'"
     ]
    }
   ],
   "source": [
    "messages = [\"CRISP Rochester\"]\n",
    "result = model.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c68cee-fbb1-4530-a537-2d86f3419e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc51368-d3b3-4d0d-a5a4-5231db9eebe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygraphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m\n\u001b[1;32m      2\u001b[0m Image(model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_png())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'"
     ]
    }
   ],
   "source": [
    "import pygraphviz\n",
    "Image(model.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3de34b-8adc-4d68-b32b-8113f8407822",
   "metadata": {},
   "source": [
    "# Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e61cb0-f772-4ba6-8e85-f11e894baa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the pdf \n",
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5342682-ce1f-44f2-9ebe-99b43d9db9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone()\n",
    "\n",
    "pc.create_index(\n",
    "    name=\"quickstart\",\n",
    "    dimension=5120, # Lamma 2 embedding dim\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGu",
   "language": "python",
   "name": "ragu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
