{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90858b08",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ceefa559-cf67-4cbb-970d-84661c36c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "env = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f7bb8f8-33b5-404b-984a-77e515536ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_openai import ChatOpenAI\n",
    "from IPython.display import Image\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3185a17",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c130533c-6cd0-414d-ad28-623e4ab6735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Dict, Any\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24b799f-7f80-4ffa-a3d5-c116aa222b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /home/rwalling/anaconda3/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/rwalling/.local/lib/python3.10/site-packages (from pypdf) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc3923a2-c5d3-4384-9d74-f98813334c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfReader\n",
    "import tiktoken\n",
    "import requests\n",
    "\n",
    "# Healper method to read PDF files\n",
    "def read_pdf(file_path) -> str:\n",
    "    output = []\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                output.append(page.extract_text())\n",
    "\n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "\n",
    "# Search for relevant menu\n",
    "search_tool = TavilySearchResults()\n",
    "\n",
    "# Scrape menu data\n",
    "class ScrapeInput(BaseModel):\n",
    "    url: str = Field(description=\"the URL of the menu page\")\n",
    "    url: str = Field(description=\"the URL of the menu page\")\n",
    "\n",
    "@tool(\"scrape_pdf\", args_schema=ScrapeInput, return_direct=True)\n",
    "def scrape_pdf(url: str):\n",
    "    \"\"\"Scrape a webpage that may include links to a restaurants current menu and return the links\"\"\"\n",
    "    pdf_links = []\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.find_all('a')\n",
    "        pdf_links = [link.get('href') for link in links if link.get('href').endswith('.pdf')]\n",
    "    except Exception as e:\n",
    "        print(f\"failed to scrape {url} ERROR: {e}\")\n",
    "    return pdf_links\n",
    "\n",
    "@tool(\"scrape_text\", args_schema=ScrapeInput, return_direct=True)\n",
    "def scrape_text(url: str):\n",
    "    \"\"\"Scrape the text directly from a website\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        # Get text\n",
    "        text = soup.get_text(separator=' ')\n",
    "        # Break into lines and remove leading and trailing space on each\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # Break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # Drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"failed to scrape {url} ERROR: {e}\")\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "class DownloadInput(BaseModel):\n",
    "    download_url: str = Field(description=\"the URL of the menu pdf\")\n",
    "    filename: str = Field(description=\"the name of the restaurants\")\n",
    "\n",
    "@tool(\"download-pdf\", args_schema=DownloadInput, return_direct=True)\n",
    "def download_pdf(download_url: str, filename: str):\n",
    "    \"\"\"Download a pdf file from a given url and filename\"\"\"\n",
    "    response = urlopen(download_url)\n",
    "    file = open(\"pdf/\"+filename+\".pdf\", 'wb')\n",
    "    file.write(response.read())\n",
    "    file.close()\n",
    "\n",
    "# Tokenize the pdf \n",
    "class upsertInput(BaseModel):\n",
    "    file_path: str = Field(description=\"The filepath to the pdf in the format of pdf/filename.pdf\")\n",
    "    restaurant_name: str = Field(description=\"The name of the restaurant\")\n",
    "    location: str = Field(description=\"The location of the restaurant\")\n",
    "\n",
    "\n",
    "@tool(\"upsert-pdf\", args_schema=upsertInput, return_direct=True)\n",
    "def upsert_pdf(file_path, restaurant_name, location):\n",
    "    \"\"\"Upsert the pdf_file with metadata for vector search\"\"\"\n",
    "    # Create the text splitter\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    text_splitter_semantic = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "    \n",
    "    # Split the input text\n",
    "    file = read_pdf(file_path)\n",
    "    docs = text_splitter_semantic.create_documents(file)\n",
    "    \n",
    "    # Add metadata \n",
    "    metadata = {\n",
    "        \"name\": restaurant_name,\n",
    "        \"location\": location,\n",
    "    }\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.metadata = metadata\n",
    "        \n",
    "    # Point to our Pinecone index\n",
    "    pc = Pinecone()\n",
    "    index = pc.Index(\"paprika-ragu\")\n",
    "    \n",
    "    # Upsert the data\n",
    "    PineconeVectorStore.from_documents(docs, embeddings, index_name=\"paprika-ragu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b4ee54d-15aa-466b-85ca-eab668d1f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [search_tool, scrape_pdf, scrape_text, download_pdf, upsert_pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be6353-a2a7-4c56-aad4-7afc3a42efd1",
   "metadata": {},
   "source": [
    "# RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc4005b8-599b-4a87-8a68-5182b5d93bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    # Define the model\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.openai_inference)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    # Check if llm requires action \n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    # Run a tool ordered by the model\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n",
    "    \n",
    "    # Invokes the current message chain\n",
    "    def openai_inference(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a1fe975-3bbc-4814-b979-048899d6d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are tasked with searching for a given restaurant menu, downloading the menu, and the upserting the menu to a vector database. \n",
    "----------\n",
    "You will first search for the restaurant menu, you will either download the PDF file or scrape the text directly from the page if a PDF is not available.\n",
    "ONLY scrape menu data, this includes foods that the restaurant offers and DOES NOT include information about the restaurant itself or the menu iteself.\n",
    "Use the search tool to find the url of the menu, if you cannot find the pdf url directly use the scrape_pdf tool to scrape webpages for the url.\n",
    "----------\n",
    "Once you have downloaded or scraped the menu, you will then upsert it into our vector database. \n",
    "You will provide links to the pdf_file holding the menu, aswell as the name of the restaurant and the location of the restaurant.\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "\n",
    "model = Model(llm, tools, system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ad031-e7ff-410f-baf1-bf70c1d7cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'CRISP Rochester menu'}, 'id': 'call_rBj9wXpQ2VuuLw08yBhsgi3A', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "Calling: {'name': 'scrape_pdf', 'args': {'url': 'https://crisprochester.com/menus/'}, 'id': 'call_6S6rg8m9B8vM4FLEtQsXlgi0', 'type': 'tool_call'}\n",
      "Calling: {'name': 'scrape_text', 'args': {'url': 'https://crisprochester.com/menus/'}, 'id': 'call_F5fj3CkJBpuIwjubHVQqinL9', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "Calling: {'name': 'download-pdf', 'args': {'download_url': 'https://crisprochester.com/wp-content/uploads/Lunch-and-Dinner.pdf', 'filename': 'CRISP_Rochester_Lunch_and_Dinner'}, 'id': 'call_cUva33fbSbiEMYedzZLM0Kbq', 'type': 'tool_call'}\n",
      "Calling: {'name': 'download-pdf', 'args': {'download_url': 'https://crisprochester.com/wp-content/uploads/Brunch.pdf', 'filename': 'CRISP_Rochester_Brunch'}, 'id': 'call_cpv9cqJabJOi2XjzionH09hh', 'type': 'tool_call'}\n",
      "Calling: {'name': 'download-pdf', 'args': {'download_url': 'https://crisprochester.com/wp-content/uploads/Desserts.pdf', 'filename': 'CRISP_Rochester_Desserts'}, 'id': 'call_vmoGPcIHjxMOkbKT5qo2Bl7j', 'type': 'tool_call'}\n",
      "Calling: {'name': 'download-pdf', 'args': {'download_url': 'https://crisprochester.com/wp-content/uploads/Cocktails-and-Mocktails.pdf', 'filename': 'CRISP_Rochester_Cocktails_and_Mocktails'}, 'id': 'call_Tf1B4B3LbufiwHooyQXHJlXh', 'type': 'tool_call'}\n",
      "Calling: {'name': 'download-pdf', 'args': {'download_url': 'https://crisprochester.com/wp-content/uploads/Wine.pdf', 'filename': 'CRISP_Rochester_Wine'}, 'id': 'call_xm84YBwkcswisWbuYgu2pWG8', 'type': 'tool_call'}\n",
      "Calling: {'name': 'download-pdf', 'args': {'download_url': 'https://crisprochester.com/wp-content/uploads/Beer-1.pdf', 'filename': 'CRISP_Rochester_Beer'}, 'id': 'call_DrT49N6y7KIiwGVrvICvd3dl', 'type': 'tool_call'}\n",
      "Back to the model!\n",
      "Calling: {'name': 'upsert-pdf', 'args': {'file_path': 'pdf/CRISP_Rochester_Lunch_and_Dinner.pdf', 'restaurant_name': 'CRISP', 'location': 'Rochester, NY'}, 'id': 'call_t250cUTWkbIA9NelkcHDb7jW', 'type': 'tool_call'}\n",
      "Calling: {'name': 'upsert-pdf', 'args': {'file_path': 'pdf/CRISP_Rochester_Brunch.pdf', 'restaurant_name': 'CRISP', 'location': 'Rochester, NY'}, 'id': 'call_VQLOEW3UKjhlWge4hJs1lyI1', 'type': 'tool_call'}\n",
      "Calling: {'name': 'upsert-pdf', 'args': {'file_path': 'pdf/CRISP_Rochester_Desserts.pdf', 'restaurant_name': 'CRISP', 'location': 'Rochester, NY'}, 'id': 'call_WiA3ZwFARHHPleCeeboPubXi', 'type': 'tool_call'}\n"
     ]
    }
   ],
   "source": [
    "messages = [\"CRISP Rochester\"]\n",
    "result = model.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3c68cee-fbb1-4530-a537-2d86f3419e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc51368-d3b3-4d0d-a5a4-5231db9eebe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pygraphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygraphviz\u001b[39;00m\n\u001b[1;32m      2\u001b[0m Image(model\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_png())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'"
     ]
    }
   ],
   "source": [
    "import pygraphviz\n",
    "Image(model.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3de34b-8adc-4d68-b32b-8113f8407822",
   "metadata": {},
   "source": [
    "# Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9283e36a-57f4-4801-8b9f-4d767573acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-openai \\\n",
    "    langchain \\\n",
    "    langchain-community \\\n",
    "    pinecone-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867c06ae-438b-45cb-9e6c-61618ceaa6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_pdf(file_path) -> str:\n",
    "    output = []\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                output.append(page.extract_text())\n",
    "\n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF file: {e}\")\n",
    "\n",
    "# Replace 'your_pdf_file.pdf' with the path to your PDF file\n",
    "file = read_pdf('pdf/menu.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43386c1c-ce36-4930-a8a6-9d3dd3585705",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter_semantic.create_documents(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d0ee034-b7f7-447c-b3b7-9125c1d70954",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5342682-ce1f-44f2-9ebe-99b43d9db9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e837146-71a6-4740-a095-d4fc85044c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"paprika-ragu\"\n",
    "\n",
    "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
    "\n",
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )\n",
    "    while not pc.describe_index(index_name).status[\"ready\"]:\n",
    "        time.sleep(1)\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56a548b5-5fe7-4ef2-ad9b-590b5a42a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dfbb485-61fa-4d0c-a8ee-9e59e3e1e588",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoes CRISP Rochester fit a vegan diet?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mdocsearch\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(query)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docsearch' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"Does CRISP Rochester fit a vegan diet?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fb515-9b8b-41b6-91da-a58f267f0f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ragu",
   "language": "python",
   "name": "ragu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
